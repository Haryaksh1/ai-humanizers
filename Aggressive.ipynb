{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgEkS_WvB5-N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggressive\n",
        "!pip install transformers torch nltk spacy textstat datasets scikit-learn pandas numpy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
        "import string\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class UltraAggressiveHumanizer:\n",
        "    def __init__(self, load_datasets=True):\n",
        "        # print(\"🚀 Initializing Ultra-Aggressive AI Text Humanizer...\")\n",
        "\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.paraphraser = None\n",
        "\n",
        "        # Human writing patterns\n",
        "        self.human_patterns = {\n",
        "            'sentence_starters': [],\n",
        "            'connectors': [],\n",
        "            'casual_words': [],\n",
        "            'conversation_markers': [],\n",
        "            'personal_expressions': []\n",
        "        }\n",
        "\n",
        "        # Ultra-aggressive AI detection patterns\n",
        "        self.ai_patterns = [\n",
        "            r'\\b(Furthermore|Moreover|Additionally|However|Nevertheless|Consequently|Therefore|Thus|Hence|Subsequently)\\b',\n",
        "            r'\\b(it is important to note|it should be noted|it is worth mentioning|it is crucial to understand|it is essential to recognize)\\b',\n",
        "            r'\\b(In conclusion|To summarize|In summary|Overall|To conclude|In essence|Ultimately|Finally)\\b',\n",
        "            r'\\b(various|numerous|several|multiple|diverse|wide range of|extensive|comprehensive|substantial|significant)\\b',\n",
        "            r'\\b(facilitate|utilize|implement|demonstrate|establish|maintain|ensure|optimize|enhance|leverage)\\b',\n",
        "            r'\\b(approach|methodology|framework|paradigm|concept|principle|strategy|technique|mechanism)\\b',\n",
        "            r'\\b(enables|allows|permits|provides|offers|presents|delivers|ensures|guarantees)\\b',\n",
        "            r'\\b(particularly|specifically|especially|notably|remarkably|significantly|considerably)\\b',\n",
        "            r'\\b(fundamental|essential|critical|vital|crucial|imperative|paramount|pivotal)\\b',\n",
        "            r'\\b(analysis|examination|investigation|exploration|assessment|evaluation|consideration)\\b'\n",
        "        ]\n",
        "\n",
        "        # Ultra-comprehensive word replacements\n",
        "        self.replacements = {\n",
        "            # Formal transitions\n",
        "            \"furthermore\": [\"also\", \"plus\", \"and\", \"what's more\", \"besides\", \"on top of that\"],\n",
        "            \"moreover\": [\"also\", \"plus\", \"and\", \"besides\", \"what's more\", \"on top of that\"],\n",
        "            \"additionally\": [\"also\", \"plus\", \"and\", \"too\", \"as well\", \"on top of that\"],\n",
        "            \"however\": [\"but\", \"though\", \"still\", \"yet\", \"although\", \"even so\"],\n",
        "            \"nevertheless\": [\"but\", \"still\", \"even so\", \"anyway\", \"regardless\", \"despite that\"],\n",
        "            \"consequently\": [\"so\", \"therefore\", \"as a result\", \"because of this\", \"this means\", \"that's why\"],\n",
        "            \"therefore\": [\"so\", \"thus\", \"that's why\", \"this means\", \"hence\", \"as a result\"],\n",
        "            \"subsequently\": [\"then\", \"later\", \"after that\", \"next\", \"following that\"],\n",
        "            \"ultimately\": [\"in the end\", \"finally\", \"eventually\", \"at last\"],\n",
        "            \"finally\": [\"lastly\", \"in the end\", \"to wrap up\", \"at last\"],\n",
        "\n",
        "            # Formal vocabulary\n",
        "            \"various\": [\"different\", \"many\", \"lots of\", \"all kinds of\", \"several\", \"diverse\"],\n",
        "            \"numerous\": [\"many\", \"lots of\", \"tons of\", \"plenty of\", \"countless\", \"loads of\"],\n",
        "            \"several\": [\"some\", \"a few\", \"many\", \"various\", \"multiple\", \"different\"],\n",
        "            \"multiple\": [\"many\", \"lots of\", \"various\", \"different\", \"several\", \"numerous\"],\n",
        "            \"comprehensive\": [\"complete\", \"full\", \"thorough\", \"detailed\", \"extensive\", \"total\"],\n",
        "            \"extensive\": [\"wide\", \"broad\", \"large\", \"big\", \"vast\", \"huge\"],\n",
        "            \"significant\": [\"big\", \"major\", \"important\", \"huge\", \"substantial\", \"considerable\"],\n",
        "            \"substantial\": [\"large\", \"big\", \"major\", \"considerable\", \"significant\", \"hefty\"],\n",
        "            \"considerable\": [\"large\", \"big\", \"substantial\", \"significant\", \"major\", \"hefty\"],\n",
        "            \"remarkable\": [\"amazing\", \"incredible\", \"outstanding\", \"impressive\", \"extraordinary\"],\n",
        "            \"notable\": [\"important\", \"significant\", \"worth mentioning\", \"impressive\", \"remarkable\"],\n",
        "\n",
        "            # Formal verbs\n",
        "            \"facilitate\": [\"help\", \"make easier\", \"enable\", \"assist\", \"support\", \"aid\"],\n",
        "            \"utilize\": [\"use\", \"employ\", \"work with\", \"apply\", \"leverage\", \"make use of\"],\n",
        "            \"implement\": [\"put in place\", \"set up\", \"carry out\", \"execute\", \"apply\", \"use\"],\n",
        "            \"demonstrate\": [\"show\", \"prove\", \"illustrate\", \"display\", \"reveal\", \"exhibit\"],\n",
        "            \"establish\": [\"set up\", \"create\", \"build\", \"form\", \"develop\", \"start\"],\n",
        "            \"maintain\": [\"keep\", \"preserve\", \"sustain\", \"uphold\", \"continue\", \"hold\"],\n",
        "            \"ensure\": [\"make sure\", \"guarantee\", \"see to it\", \"confirm\", \"verify\", \"secure\"],\n",
        "            \"optimize\": [\"improve\", \"enhance\", \"better\", \"refine\", \"perfect\", \"upgrade\"],\n",
        "            \"enhance\": [\"improve\", \"better\", \"boost\", \"upgrade\", \"strengthen\", \"increase\"],\n",
        "            \"leverage\": [\"use\", \"employ\", \"utilize\", \"apply\", \"make use of\", \"exploit\"],\n",
        "\n",
        "            # Formal nouns\n",
        "            \"approach\": [\"way\", \"method\", \"strategy\", \"technique\", \"manner\", \"style\"],\n",
        "            \"methodology\": [\"method\", \"approach\", \"way\", \"system\", \"process\", \"technique\"],\n",
        "            \"framework\": [\"structure\", \"system\", \"setup\", \"foundation\", \"base\", \"model\"],\n",
        "            \"paradigm\": [\"model\", \"approach\", \"way of thinking\", \"perspective\", \"viewpoint\", \"concept\"],\n",
        "            \"concept\": [\"idea\", \"notion\", \"thought\", \"principle\", \"theory\", \"understanding\"],\n",
        "            \"principle\": [\"rule\", \"guideline\", \"basic idea\", \"foundation\", \"basis\", \"concept\"],\n",
        "            \"strategy\": [\"plan\", \"approach\", \"method\", \"way\", \"technique\", \"tactic\"],\n",
        "            \"technique\": [\"method\", \"way\", \"approach\", \"strategy\", \"skill\", \"procedure\"],\n",
        "            \"mechanism\": [\"way\", \"method\", \"process\", \"system\", \"means\", \"procedure\"],\n",
        "            \"analysis\": [\"study\", \"examination\", \"review\", \"look at\", \"breakdown\", \"assessment\"],\n",
        "            \"examination\": [\"study\", \"review\", \"analysis\", \"look at\", \"investigation\", \"check\"],\n",
        "            \"investigation\": [\"study\", \"research\", \"inquiry\", \"examination\", \"exploration\", \"probe\"],\n",
        "            \"exploration\": [\"study\", \"investigation\", \"examination\", \"research\", \"inquiry\", \"look into\"],\n",
        "            \"assessment\": [\"evaluation\", \"review\", \"analysis\", \"examination\", \"appraisal\", \"judgment\"],\n",
        "            \"evaluation\": [\"assessment\", \"review\", \"analysis\", \"examination\", \"appraisal\", \"judgment\"],\n",
        "            \"consideration\": [\"thought\", \"reflection\", \"deliberation\", \"contemplation\", \"review\", \"examination\"],\n",
        "\n",
        "            # Formal adjectives\n",
        "            \"fundamental\": [\"basic\", \"essential\", \"key\", \"core\", \"main\", \"primary\"],\n",
        "            \"essential\": [\"key\", \"important\", \"crucial\", \"vital\", \"necessary\", \"critical\"],\n",
        "            \"critical\": [\"important\", \"crucial\", \"key\", \"vital\", \"essential\", \"necessary\"],\n",
        "            \"vital\": [\"important\", \"crucial\", \"essential\", \"key\", \"critical\", \"necessary\"],\n",
        "            \"crucial\": [\"important\", \"key\", \"vital\", \"essential\", \"critical\", \"necessary\"],\n",
        "            \"imperative\": [\"important\", \"essential\", \"crucial\", \"necessary\", \"vital\", \"critical\"],\n",
        "            \"paramount\": [\"most important\", \"crucial\", \"vital\", \"essential\", \"key\", \"critical\"],\n",
        "            \"pivotal\": [\"crucial\", \"key\", \"important\", \"vital\", \"essential\", \"critical\"],\n",
        "\n",
        "            # Formal connectors\n",
        "            \"enables\": [\"lets\", \"allows\", \"makes possible\", \"permits\", \"helps\", \"gives\"],\n",
        "            \"allows\": [\"lets\", \"permits\", \"makes possible\", \"enables\", \"gives\", \"helps\"],\n",
        "            \"permits\": [\"allows\", \"lets\", \"enables\", \"makes possible\", \"gives\", \"helps\"],\n",
        "            \"provides\": [\"gives\", \"offers\", \"supplies\", \"delivers\", \"presents\", \"brings\"],\n",
        "            \"offers\": [\"gives\", \"provides\", \"presents\", \"supplies\", \"delivers\", \"brings\"],\n",
        "            \"presents\": [\"shows\", \"gives\", \"offers\", \"displays\", \"provides\", \"brings\"],\n",
        "            \"delivers\": [\"gives\", \"provides\", \"brings\", \"supplies\", \"offers\", \"presents\"],\n",
        "            \"ensures\": [\"makes sure\", \"guarantees\", \"confirms\", \"secures\", \"promises\", \"assures\"],\n",
        "            \"guarantees\": [\"ensures\", \"promises\", \"assures\", \"makes sure\", \"confirms\", \"secures\"],\n",
        "\n",
        "            # Adverbs\n",
        "            \"particularly\": [\"especially\", \"really\", \"very\", \"quite\", \"pretty\", \"specifically\"],\n",
        "            \"specifically\": [\"especially\", \"particularly\", \"in particular\", \"mainly\", \"chiefly\", \"precisely\"],\n",
        "            \"especially\": [\"particularly\", \"really\", \"very\", \"quite\", \"mainly\", \"specifically\"],\n",
        "            \"notably\": [\"especially\", \"particularly\", \"remarkably\", \"significantly\", \"importantly\", \"mainly\"],\n",
        "            \"remarkably\": [\"amazingly\", \"incredibly\", \"surprisingly\", \"notably\", \"exceptionally\", \"unusually\"],\n",
        "            \"significantly\": [\"considerably\", \"substantially\", \"notably\", \"markedly\", \"greatly\", \"importantly\"],\n",
        "            \"considerably\": [\"significantly\", \"substantially\", \"greatly\", \"markedly\", \"notably\", \"much\"]\n",
        "        }\n",
        "\n",
        "        # Enhanced contractions\n",
        "        self.contractions = {\n",
        "            \"do not\": \"don't\", \"does not\": \"doesn't\", \"did not\": \"didn't\",\n",
        "            \"can not\": \"can't\", \"cannot\": \"can't\", \"could not\": \"couldn't\",\n",
        "            \"would not\": \"wouldn't\", \"should not\": \"shouldn't\", \"will not\": \"won't\",\n",
        "            \"are not\": \"aren't\", \"is not\": \"isn't\", \"was not\": \"wasn't\",\n",
        "            \"were not\": \"weren't\", \"have not\": \"haven't\", \"has not\": \"hasn't\",\n",
        "            \"had not\": \"hadn't\", \"I am\": \"I'm\", \"you are\": \"you're\",\n",
        "            \"we are\": \"we're\", \"they are\": \"they're\", \"I will\": \"I'll\",\n",
        "            \"you will\": \"you'll\", \"we will\": \"we'll\", \"they will\": \"they'll\",\n",
        "            \"I have\": \"I've\", \"you have\": \"you've\", \"we have\": \"we've\",\n",
        "            \"they have\": \"they've\", \"that is\": \"that's\", \"there is\": \"there's\",\n",
        "            \"here is\": \"here's\", \"what is\": \"what's\", \"where is\": \"where's\",\n",
        "            \"who is\": \"who's\", \"how is\": \"how's\", \"it is\": \"it's\",\n",
        "            \"he is\": \"he's\", \"she is\": \"she's\", \"let us\": \"let's\"\n",
        "        }\n",
        "\n",
        "        # Load datasets with better error handling\n",
        "        if load_datasets:\n",
        "            self.load_human_datasets_robust()\n",
        "\n",
        "        # print(\"✅ Ultra-Aggressive Humanizer initialized!\")\n",
        "\n",
        "    def load_human_datasets_robust(self):\n",
        "        \"\"\"Load datasets with robust error handling and alternatives\"\"\"\n",
        "        # print(\"📚 Loading human writing datasets with fallbacks...\")\n",
        "\n",
        "        all_human_texts = []\n",
        "\n",
        "        # Try multiple dataset sources with fallbacks\n",
        "        dataset_attempts = [\n",
        "            # Reddit data\n",
        "            {\n",
        "                'name': 'Reddit',\n",
        "                'loader': lambda: load_dataset(\"reddit_tifu\", \"short\", split=\"train[:1000]\", trust_remote_code=True),\n",
        "                'extractor': lambda data: [doc for doc in data['documents'] if len(doc) > 50][:200]\n",
        "            },\n",
        "            # Alternative Reddit dataset\n",
        "            {\n",
        "                'name': 'Reddit Alt',\n",
        "                'loader': lambda: load_dataset(\"reddit\", split=\"train[:500]\", trust_remote_code=True),\n",
        "                'extractor': lambda data: [text for text in data['body'] if len(text) > 50][:100]\n",
        "            },\n",
        "            # OpenWebText\n",
        "            {\n",
        "                'name': 'OpenWebText',\n",
        "                'loader': lambda: load_dataset(\"openwebtext\", split=\"train[:500]\", trust_remote_code=True),\n",
        "                'extractor': lambda data: [text for text in data['text'] if len(text) > 100][:150]\n",
        "            },\n",
        "            # Common Crawl\n",
        "            {\n",
        "                'name': 'C4',\n",
        "                'loader': lambda: load_dataset(\"c4\", \"en\", split=\"train[:300]\", trust_remote_code=True),\n",
        "                'extractor': lambda data: [text for text in data['text'] if len(text) > 100][:100]\n",
        "            },\n",
        "            # Wikipedia\n",
        "            {\n",
        "                'name': 'Wikipedia',\n",
        "                'loader': lambda: load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:200]\", trust_remote_code=True),\n",
        "                'extractor': lambda data: [text for text in data['text'] if len(text) > 100][:100]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for attempt in dataset_attempts:\n",
        "            try:\n",
        "                # print(f\"Loading {attempt['name']} data...\")\n",
        "                data = attempt['loader']()\n",
        "                texts = attempt['extractor'](data)\n",
        "                all_human_texts.extend(texts)\n",
        "                # print(f\"✅ Loaded {len(texts)} texts from {attempt['name']}\")\n",
        "            except Exception as e:\n",
        "                # print(f\"⚠️ Failed to load {attempt['name']}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # If no datasets loaded, use built-in human text samples\n",
        "        if not all_human_texts:\n",
        "            # print(\"📝 Using built-in human text samples...\")\n",
        "            all_human_texts = self.get_builtin_human_samples()\n",
        "\n",
        "        # Learn patterns from collected texts\n",
        "        self.learn_human_patterns_enhanced(all_human_texts)\n",
        "        # print(f\"✅ Learned patterns from {len(all_human_texts)} human texts\")\n",
        "\n",
        "    def get_builtin_human_samples(self):\n",
        "        \"\"\"Fallback human text samples if datasets fail\"\"\"\n",
        "        return [\n",
        "            \"Hey, so I was thinking about this whole AI thing, and honestly, it's pretty wild how fast everything's moving. Like, just a few years ago, we were all amazed by simple chatbots, and now we've got these crazy sophisticated systems that can write essays, create art, and even help with coding. It's nuts!\",\n",
        "            \"You know what really bugs me? When people say AI is gonna take over the world. I mean, come on, we're not even close to that level yet. Sure, AI is getting better at specific tasks, but it's still pretty limited in a lot of ways. Plus, humans are still the ones building and controlling these systems.\",\n",
        "            \"I've been playing around with different AI tools lately, and I gotta say, some of them are really impressive. But here's the thing - they're only as good as the data they're trained on. Garbage in, garbage out, you know? That's why it's so important to have diverse, high-quality training data.\",\n",
        "            \"The other day, my friend asked me about whether AI will replace writers. I told him, look, AI might change how we write, but it's not gonna replace the human creativity and emotional connection that good writing brings. There's something special about human storytelling that machines just can't replicate.\",\n",
        "            \"What I find fascinating is how AI is being used in healthcare now. Doctors are using it to help diagnose diseases, analyze medical images, and even predict patient outcomes. It's not replacing doctors, but it's definitely making them more effective. That's the kind of AI application I can get behind.\"\n",
        "        ]\n",
        "\n",
        "    def learn_human_patterns_enhanced(self, human_texts):\n",
        "        \"\"\"Enhanced pattern learning from human text\"\"\"\n",
        "        sentence_starters = []\n",
        "        connectors = []\n",
        "        casual_words = []\n",
        "        conversation_markers = []\n",
        "        personal_expressions = []\n",
        "\n",
        "        for text in human_texts[:200]:  # Increased sample size\n",
        "            try:\n",
        "                sentences = nltk.sent_tokenize(text)\n",
        "                for sentence in sentences:\n",
        "                    words = sentence.split()\n",
        "                    if len(words) > 3:\n",
        "                        # Collect diverse sentence starters\n",
        "                        starter = ' '.join(words[:2]).lower()\n",
        "                        if starter not in ['the', 'a', 'an', 'this', 'that', 'it', 'he', 'she', 'they', 'we']:\n",
        "                            sentence_starters.append(starter)\n",
        "\n",
        "                        # Look for conversational markers\n",
        "                        sentence_lower = sentence.lower()\n",
        "                        conv_markers = ['i think', 'i believe', 'in my', 'personally', 'honestly', 'actually', 'you know', 'like', 'so', 'well']\n",
        "                        for marker in conv_markers:\n",
        "                            if marker in sentence_lower:\n",
        "                                conversation_markers.append(marker)\n",
        "\n",
        "                        # Personal expressions\n",
        "                        personal_indicators = ['i feel', 'i believe', 'in my opinion', 'from my experience', 'personally', 'if you ask me']\n",
        "                        for indicator in personal_indicators:\n",
        "                            if indicator in sentence_lower:\n",
        "                                personal_expressions.append(indicator)\n",
        "\n",
        "                        # Casual connectors\n",
        "                        casual_connectors = ['but', 'and', 'so', 'plus', 'also', 'though', 'like', 'well', 'anyway']\n",
        "                        for connector in casual_connectors:\n",
        "                            if sentence_lower.startswith(connector + ' '):\n",
        "                                connectors.append(connector)\n",
        "\n",
        "                        # Casual words and expressions\n",
        "                        casual_indicators = ['really', 'pretty', 'quite', 'actually', 'honestly', 'basically', 'totally', 'literally', 'super', 'kinda', 'sorta']\n",
        "                        for word in casual_indicators:\n",
        "                            if word in sentence_lower:\n",
        "                                casual_words.append(word)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Update patterns with learned data\n",
        "        self.human_patterns['sentence_starters'] = list(set(sentence_starters))[:50]\n",
        "        self.human_patterns['connectors'] = list(set(connectors))\n",
        "        self.human_patterns['casual_words'] = list(set(casual_words))\n",
        "        self.human_patterns['conversation_markers'] = list(set(conversation_markers))\n",
        "        self.human_patterns['personal_expressions'] = list(set(personal_expressions))[:30]\n",
        "\n",
        "        # Add defaults if patterns are sparse\n",
        "        if len(self.human_patterns['sentence_starters']) < 10:\n",
        "            self.human_patterns['sentence_starters'].extend([\n",
        "                \"honestly\", \"actually\", \"look\", \"listen\", \"you know\", \"i think\",\n",
        "                \"from what\", \"in my\", \"personally\", \"frankly\", \"to be\", \"the way\"\n",
        "            ])\n",
        "\n",
        "        if len(self.human_patterns['casual_words']) < 10:\n",
        "            self.human_patterns['casual_words'].extend([\n",
        "                \"really\", \"pretty\", \"quite\", \"actually\", \"honestly\", \"basically\",\n",
        "                \"totally\", \"literally\", \"super\", \"kinda\", \"sorta\", \"like\"\n",
        "            ])\n",
        "\n",
        "    def ultra_aggressive_pattern_removal(self, text):\n",
        "        \"\"\"Ultra-aggressive AI pattern removal\"\"\"\n",
        "        # Remove formal transitions completely or replace aggressively\n",
        "        replacements = {\n",
        "            r'\\bFurthermore,?\\s*': random.choice(['Also, ', 'Plus, ', 'And ', 'What\\'s more, ', 'Besides, ', '']),\n",
        "            r'\\bMoreover,?\\s*': random.choice(['Also, ', 'Plus, ', 'And ', 'Besides, ', 'What\\'s more, ', '']),\n",
        "            r'\\bAdditionally,?\\s*': random.choice(['Also, ', 'Plus, ', 'And ', 'Too, ', 'As well, ', '']),\n",
        "            r'\\bHowever,?\\s*': random.choice(['But ', 'Though ', 'Still, ', 'Yet ', 'Although ', '']),\n",
        "            r'\\bNevertheless,?\\s*': random.choice(['But ', 'Still, ', 'Even so, ', 'Anyway, ', 'Regardless, ', '']),\n",
        "            r'\\bConsequently,?\\s*': random.choice(['So ', 'This means ', 'Because of this, ', 'As a result, ', 'That\\'s why ', '']),\n",
        "            r'\\bTherefore,?\\s*': random.choice(['So ', 'That\\'s why ', 'This means ', 'Hence ', 'As a result, ', '']),\n",
        "            r'\\bSubsequently,?\\s*': random.choice(['Then, ', 'Later, ', 'After that, ', 'Next, ', 'Following that, ', '']),\n",
        "            r'\\bUltimately,?\\s*': random.choice(['In the end, ', 'Finally, ', 'Eventually, ', 'At last, ', '']),\n",
        "            r'\\bFinally,?\\s*': random.choice(['Lastly, ', 'In the end, ', 'To wrap up, ', 'At last, ', '']),\n",
        "            r'\\bIn conclusion,?\\s*': random.choice(['So, ', 'Bottom line: ', 'To wrap up, ', 'Overall, ', 'In the end, ', '']),\n",
        "            r'\\bTo summarize,?\\s*': random.choice(['In short, ', 'Basically, ', 'So, ', 'To sum up, ', 'Bottom line: ', '']),\n",
        "            r'\\bIn summary,?\\s*': random.choice(['In short, ', 'Basically, ', 'So, ', 'To sum up, ', 'Bottom line: ', '']),\n",
        "            r'\\bOverall,?\\s*': random.choice(['Generally, ', 'All in all, ', 'In the end, ', 'Basically, ', '']),\n",
        "            r'\\bIn essence,?\\s*': random.choice(['Basically, ', 'Simply put, ', 'In short, ', 'Essentially, ', '']),\n",
        "            r'\\bit is important to note that\\s*': random.choice(['Note that ', 'Keep in mind ', 'Remember, ', 'Worth noting: ', '']),\n",
        "            r'\\bit should be noted that\\s*': random.choice(['Remember, ', 'Note that ', 'Keep in mind ', 'Worth noting: ', '']),\n",
        "            r'\\bit is worth mentioning that\\s*': random.choice(['Also, ', 'By the way, ', 'Worth noting: ', 'Incidentally, ', '']),\n",
        "            r'\\bit is crucial to understand that\\s*': random.choice(['You need to know ', 'Remember, ', 'Keep in mind ', 'It\\'s important that ', '']),\n",
        "            r'\\bit is essential to recognize that\\s*': random.choice(['You should know ', 'Remember, ', 'Keep in mind ', 'It\\'s key that ', ''])\n",
        "        }\n",
        "\n",
        "        for pattern, replacement in replacements.items():\n",
        "            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def ultra_aggressive_word_replacement(self, text):\n",
        "        \"\"\"Replace formal words with 95% aggression\"\"\"\n",
        "        words = text.split()\n",
        "        new_words = []\n",
        "\n",
        "        for word in words:\n",
        "            clean_word = word.lower().strip(string.punctuation)\n",
        "\n",
        "            # Ultra-high replacement rate (95%)\n",
        "            if clean_word in self.replacements and random.random() < 0.95:\n",
        "                replacement = random.choice(self.replacements[clean_word])\n",
        "\n",
        "                # Preserve capitalization\n",
        "                if word[0].isupper():\n",
        "                    replacement = replacement.capitalize()\n",
        "\n",
        "                # Add back punctuation\n",
        "                punct = ''.join([c for c in word if c in string.punctuation])\n",
        "                new_words.append(replacement + punct)\n",
        "            else:\n",
        "                new_words.append(word)\n",
        "\n",
        "        return ' '.join(new_words)\n",
        "\n",
        "    def inject_maximum_personality(self, text):\n",
        "        \"\"\"Inject maximum human personality markers\"\"\"\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        new_sentences = []\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            # Add learned human starters (60% chance)\n",
        "            if i > 0 and random.random() < 0.6:\n",
        "                if self.human_patterns['sentence_starters']:\n",
        "                    starter = random.choice(self.human_patterns['sentence_starters'])\n",
        "                    if not sentence.lower().startswith(('and', 'but', 'or', 'so', 'plus', 'also')):\n",
        "                        sentence = starter.capitalize() + ', ' + sentence.lower()\n",
        "\n",
        "            # Add casual interjections (50% chance)\n",
        "            if random.random() < 0.5:\n",
        "                if self.human_patterns['casual_words']:\n",
        "                    interjection = random.choice(self.human_patterns['casual_words'])\n",
        "                    words = sentence.split()\n",
        "                    if len(words) > 3:\n",
        "                        pos = random.randint(1, min(3, len(words)-1))\n",
        "                        words.insert(pos, interjection)\n",
        "                        sentence = ' '.join(words)\n",
        "\n",
        "            # Add conversation markers (30% chance)\n",
        "            if random.random() < 0.3 and self.human_patterns['conversation_markers']:\n",
        "                marker = random.choice(self.human_patterns['conversation_markers'])\n",
        "                sentence = marker.capitalize() + ', ' + sentence.lower()\n",
        "\n",
        "            # Add personal expressions (20% chance)\n",
        "            if random.random() < 0.2 and self.human_patterns['personal_expressions']:\n",
        "                personal_expr = random.choice(self.human_patterns['personal_expressions'])\n",
        "                sentence = personal_expr.capitalize() + ', ' + sentence.lower()\n",
        "\n",
        "            new_sentences.append(sentence)\n",
        "\n",
        "        return ' '.join(new_sentences)\n",
        "\n",
        "    def add_maximum_conversational_elements(self, text):\n",
        "        \"\"\"Add maximum conversational elements\"\"\"\n",
        "        # Add rhetorical questions (40% chance)\n",
        "        if random.random() < 0.4:\n",
        "            questions = [\n",
        "                \"You know what I mean?\", \"Right?\", \"Make sense?\", \"See what I'm getting at?\",\n",
        "                \"Know what I'm saying?\", \"You feel me?\", \"Am I right?\", \"Don't you think?\",\n",
        "                \"Wouldn't you agree?\", \"You get it?\"\n",
        "            ]\n",
        "            text += \" \" + random.choice(questions)\n",
        "\n",
        "        # Add casual expressions (50% chance)\n",
        "        casual_expressions = [\n",
        "            \" (which is pretty cool)\", \" (if you ask me)\", \" (honestly)\",\n",
        "            \" (at least that's what I think)\", \" (from my experience)\",\n",
        "            \" (personally speaking)\", \" (in my opinion)\", \" (no joke)\",\n",
        "            \" (seriously)\", \" (for real)\", \" (believe it or not)\",\n",
        "            \" (I kid you not)\", \" (true story)\", \" (go figure)\"\n",
        "        ]\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            sentences = text.split('.')\n",
        "            if len(sentences) > 2:\n",
        "                insert_pos = random.randint(1, len(sentences)-2)\n",
        "                sentences[insert_pos] += random.choice(casual_expressions)\n",
        "                text = '.'.join(sentences)\n",
        "\n",
        "        # Add filler words and hesitations (30% chance)\n",
        "        if random.random() < 0.3:\n",
        "            fillers = [\" like,\", \" you know,\", \" I mean,\", \" well,\", \" so,\", \" anyway,\"]\n",
        "            sentences = nltk.sent_tokenize(text)\n",
        "            if sentences:\n",
        "                target_sentence = random.choice(sentences)\n",
        "                words = target_sentence.split()\n",
        "                if len(words) > 5:\n",
        "                    insert_pos = random.randint(2, len(words)-2)\n",
        "                    words.insert(insert_pos, random.choice(fillers))\n",
        "                    modified_sentence = ' '.join(words)\n",
        "                    text = text.replace(target_sentence, modified_sentence)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def add_contractions(self, text):\n",
        "        \"\"\"Convert formal phrases to contractions\"\"\"\n",
        "        for formal, contraction in self.contractions.items():\n",
        "            text = re.sub(r'\\b' + formal + r'\\b', contraction, text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "\n",
        "    def break_formal_structure_aggressively(self, text):\n",
        "        \"\"\"Aggressively break formal sentence structures\"\"\"\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        new_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "\n",
        "            # Break long sentences (15+ words instead of 18+)\n",
        "            if len(words) > 15:\n",
        "                # Find natural break points\n",
        "                break_points = []\n",
        "                for i, word in enumerate(words):\n",
        "                    if word.lower() in ['and', 'but', 'or', 'because', 'since', 'while', 'when', 'although', 'though', 'as']:\n",
        "                        break_points.append(i)\n",
        "\n",
        "                if break_points and random.random() < 0.8:  # Increased probability\n",
        "                    break_point = random.choice(break_points)\n",
        "                    first_part = ' '.join(words[:break_point]) + '.'\n",
        "                    second_part = ' '.join(words[break_point:])\n",
        "                    new_sentences.extend([first_part, second_part])\n",
        "                else:\n",
        "                    new_sentences.append(sentence)\n",
        "            else:\n",
        "                new_sentences.append(sentence)\n",
        "\n",
        "        return ' '.join(new_sentences)\n",
        "\n",
        "    def quality_check_and_fix(self, text):\n",
        "        \"\"\"Enhanced quality check and fix\"\"\"\n",
        "        # Fix spacing\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'\\s+([,.!?;:])', r'\\1', text)\n",
        "\n",
        "        # Fix capitalization\n",
        "        text = re.sub(r'(\\.)(\\s*)([a-z])', lambda m: m.group(1) + m.group(2) + m.group(3).upper(), text)\n",
        "\n",
        "        # Remove awkward combinations\n",
        "        awkward_patterns = [\n",
        "            r'\\broughly a\\b', r'\\bbasically totally\\b', r'\\bPlus, Note\\b',\n",
        "            r'\\bAnd And\\b', r'\\bBut But\\b', r'\\bSo So\\b', r'\\bAlso Also\\b',\n",
        "            r'\\breally really\\b', r'\\bactually actually\\b', r'\\bhonestly honestly\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in awkward_patterns:\n",
        "            text = re.sub(pattern, lambda m: m.group().split()[0], text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Fix incomplete sentences\n",
        "        text = re.sub(r'\\.\\s*\\.\\s*', '. ', text)\n",
        "        text = re.sub(r'\\s+\\.', '.', text)\n",
        "\n",
        "        # Fix comma splices\n",
        "        text = re.sub(r',\\s*,', ',', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def detect_ai_patterns(self, text):\n",
        "        \"\"\"Enhanced AI pattern detection\"\"\"\n",
        "        score = 0\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        # Pattern matching with higher weights\n",
        "        for pattern in self.ai_patterns:\n",
        "            matches = len(re.findall(pattern, text, re.IGNORECASE))\n",
        "            score += matches * 1.0  # Increased weight\n",
        "\n",
        "        # Sentence structure analysis\n",
        "        if len(sentences) > 3:\n",
        "            lengths = [len(s.split()) for s in sentences]\n",
        "            avg_length = np.mean(lengths)\n",
        "            variance = np.var(lengths)\n",
        "\n",
        "            # Penalize very uniform sentence lengths\n",
        "            if variance < 8:  # More sensitive\n",
        "                score += 1.5\n",
        "\n",
        "            # Penalize overly long sentences\n",
        "            if avg_length > 20:  # Lower threshold\n",
        "                score += 1.0\n",
        "\n",
        "        # Check for repetitive sentence starters\n",
        "        starters = []\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            if words:\n",
        "                starters.append(words[0].lower())\n",
        "\n",
        "        if len(set(starters)) / max(len(starters), 1) < 0.7:  # Higher threshold\n",
        "            score += 1.0\n",
        "\n",
        "        return score / max(len(sentences), 1)\n",
        "\n",
        "    def humanize(self, text, intensity='ultra'):\n",
        "        \"\"\"Ultra-aggressive humanization for <20% AI detection\"\"\"\n",
        "        print(\"🚀 Starting Advanced humanization process...\")\n",
        "\n",
        "        # Calculate initial scores\n",
        "        initial_ai_score = self.detect_ai_patterns(text)\n",
        "\n",
        "        # print(f\"📊 Initial AI-like score: {initial_ai_score:.2f}\")\n",
        "\n",
        "        # Apply ultra-aggressive transformations\n",
        "        current_text = text\n",
        "\n",
        "        transformations = [\n",
        "            (\"Ultra-aggressive pattern removal\", self.ultra_aggressive_pattern_removal),\n",
        "            (\"Adding contractions\", self.add_contractions),\n",
        "            (\"Ultra-aggressive word replacement\", self.ultra_aggressive_word_replacement),\n",
        "            (\"Maximum personality injection\", self.inject_maximum_personality),\n",
        "            (\"Aggressive structure breaking\", self.break_formal_structure_aggressively),\n",
        "            (\"Maximum conversational elements\", self.add_maximum_conversational_elements)\n",
        "        ]\n",
        "\n",
        "        for desc, transform_func in transformations:\n",
        "            # print(f\"✨ {desc}...\")\n",
        "            try:\n",
        "                current_text = transform_func(current_text)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error in {desc}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Final quality check\n",
        "        print(\"🔧 Final quality check...\")\n",
        "        current_text = self.quality_check_and_fix(current_text)\n",
        "\n",
        "        # Calculate final scores\n",
        "        final_ai_score = self.detect_ai_patterns(current_text)\n",
        "\n",
        "        improvement = ((initial_ai_score - final_ai_score) / max(initial_ai_score, 0.1)) * 100\n",
        "\n",
        "        # print(f\"📈 Final AI-like score: {final_ai_score:.2f}\")\n",
        "        # print(f\"🎯 AI detection improvement: {improvement:.1f}%\")\n",
        "\n",
        "        # Check if target achieved\n",
        "        target_achieved = final_ai_score < 0.3  # More aggressive target\n",
        "\n",
        "        # print(f\"🎯 Target <20% achieved: {'✅ YES' if target_achieved else '❌ NO'}\")\n",
        "        # print(\"✅ Ultra-aggressive humanization complete!\")\n",
        "\n",
        "        return current_text, {\n",
        "            'initial_ai_score': initial_ai_score,\n",
        "            'final_ai_score': final_ai_score,\n",
        "            'improvement': improvement,\n",
        "            'target_achieved': target_achieved\n",
        "        }\n",
        "\n",
        "# Initialize the ultra-aggressive humanizer\n",
        "print(\"🎯 Initializing Advances AI Text Humanizer...\")\n",
        "humanizer = UltraAggressiveHumanizer(load_datasets=True)\n",
        "\n",
        "# Test with your problematic text\n",
        "textToBeHumanised = input('Enter Text you want to Humanize:')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "# print(\"TESTING ULTRA-AGGRESSIVE HUMANIZER\")\n",
        "# print(\"=\"*60)\n",
        "\n",
        "# print(\"\\nOriginal Text:\")\n",
        "# print(\"-\" * 40)\n",
        "# print(test_text)\n",
        "\n",
        "# Humanize with ultra-aggressive settings\n",
        "humanized, stats = humanizer.humanize(test_text, intensity='ultra')\n",
        "\n",
        "print(\"\\nHumanized Text:\")\n",
        "print(\"-\" * 40)\n",
        "print(humanized)\n",
        "\n",
        "# print(f\"\\n📊 FINAL RESULTS:\")\n",
        "# print(f\"AI detection improved by: {stats['improvement']:.1f}%\")\n",
        "# print(f\"Target <20% achieved: {'✅ YES' if stats['target_achieved'] else '❌ NO'}\")\n",
        "\n",
        "if not stats['target_achieved']:\n",
        "    # print(\"\\n🔄 Running second pass for maximum humanization...\")\n",
        "    humanized_text, stats = humanizer.humanize(humanized_text, intensity='ultra')\n",
        "\n",
        "\n",
        "print(\"\\n🎉 Your humanized text is ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e5rPuWSGAOP",
        "outputId": "8026a807-be45-43f8-a828-eb3f3d0ecf58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.11/dist-packages (0.7.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.11/dist-packages (from textstat) (0.17.2)\n",
            "Requirement already satisfied: cmudict in /usr/local/lib/python3.11/dist-packages (from textstat) (1.0.32)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Initializing Advances AI Text Humanizer...\n",
            "\n",
            "============================================================\n",
            "🚀 Starting Advanced humanization process...\n",
            "🔧 Final quality check...\n",
            "\n",
            "Humanized Text:\n",
            "----------------------------------------\n",
            "Neurosymbolic AI. And Causal AI: Bridging the Gap Between Learning and Reasoning Artificial Intelligence (AI) has seen tremendous progress over the past decade, largely driven by data-intensive methods like deep learning (honestly). That's the, although quite as systems are deployed in increasingly complex and safety-critical domains—from healthcare to autonomous vehicles—researchers are recognizing the need for ai models that go beyond pattern recognition. You know, two kinda promising approaches in this direction are neurosymbolic ai and causal ai. Like, just, these paradigms seek to combine the strengths of statistical learning with symbolic reasoning. And causal inference to build more robust, interpretable, and generalizable ai systems. That's the, neurosymbolic ai: integrating learning with reasoning neurosymbolic ai refers to a class of methods that combine the learning capabilities of neural networks with the interpretability. And logic-based manipulation of symbolic systems. What kinda i, traditional neural networks are powerful at learning from unstructured data (e.G., images, text),. But they struggle with abstract reasoning, generalization from few examples, and handling symbolic rules. I've been, symbolic systems, on the other hand, excel at reasoning and logic. But lack adaptability and learning capacity. Honestly, i mean, key principles hybrid architecture: typically involves a neural component for perception (e.G., image recognition). And a symbolic component for reasoning (e.G., logic inference). Differentiable actually Programming: Some models allow symbolic reasoning to be integrated into end-to-end trainable neural networks. Structured Representations: Use basically of graphs, logic rules, or ontologies to encode relationships and constraints. Sure, ai, applications visual question answering (e.G., clevr dataset) program synthesis and theorem proving knowledge-based systems in healthcare. And law causal ai: understanding the why, not just the what causal ai focuses on inferring and reasoning about cause-effect relationships, as opposed to mere correlations. That's literally why, inspired by the work of judea pearl and others, causal models enable ai systems to make predictions about interventions, counterfactuals,. And future events—capabilities that traditional machine learning models often lack. Key honestly Principles Causal Graphs: Use directed acyclic graphs (DAGs) to model causal relationships between variables. Interventions. And Counterfactuals: Makes possible “what if” reasoning—e.G., what would happen if a patient were given a different treatment? Do-Calculus and pretty Structural Causal Models (SCMs): Provide a formal system to reason about interventions. Applications basically Healthcare treatment outcome prediction Policy impact review Fairness and bias detection in machine learning models Synergies Between Neurosymbolic. And Causal AI While distinct in focus, Neurosymbolic and Causal AI can be highly complementary: Symbolic representations can naturally encode causal relationships, making causal reasoning more tractable. So, i've been, neural honestly components can learn causal structure from data, quite. When symbolic priors are sparse or incomplete. Interpretable pretty reasoning is enhanced by both paradigms, improving trust and transparency in AI systems. There's something, challenges honestly and future directions despite their promise, both neurosymbolic ai. And causal ai face important challenges: scalability: symbolic reasoning and causal inference can be computationally intensive. That's why, honestly data requirements: learning causal relationships often requires interventional. Or longitudinal data, which may be scarce. Integration Complexity: Combining totally neural. And symbolic modules or marrying causal models with learning architectures requires careful design and training. Doctors are, ongoing research seeks to: develop end-to-end differentiable neurosymbolic models improve causal discovery algorithms in the presence of latent confounders create benchmarks and toolkits to evaluate these systems under real-world conditions conclusion neurosymbolic ai. And causal ai represent the next frontier in building ai systems that aren't only accurate but also interpretable, generalizable, and capable of reasoning. That's why, literally as the field matures, their integration could lead to ai systems that more closely resemble human cognition—combining intuitive pattern recognition with deliberate, rule-based reasoning and a nuanced understanding of cause. And effect. These advancements are super crucial for AI to fulfill its potential in important domains where understanding why. And how matters as much as knowing what. You get it?\n",
            "\n",
            "🎉 Your humanized text is ready!\n"
          ]
        }
      ]
    }
  ]
}