{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X2pdQy2CFij"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Balanced\n",
        "\n",
        "!pip install transformers torch nltk spacy textstat datasets scikit-learn pandas numpy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
        "import string\n",
        "import gc\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class AdvancedAITextHumanizer:\n",
        "    def __init__(self, load_datasets=True):\n",
        "        print(\"ðŸš€ Initializing Advanced AI Text Humanizer...\")\n",
        "\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.paraphraser = None  # Lazy loading\n",
        "\n",
        "        # Human writing patterns learned from datasets\n",
        "        self.human_patterns = {\n",
        "            'sentence_starters': [],\n",
        "            'connectors': [],\n",
        "            'casual_words': [],\n",
        "            'conversation_markers': [],\n",
        "            'personal_expressions': []\n",
        "        }\n",
        "\n",
        "        # Enhanced AI detection patterns\n",
        "        self.ai_patterns = [\n",
        "            r'\\b(Furthermore|Moreover|Additionally|However|Nevertheless|Consequently|Therefore|Thus|Hence)\\b',\n",
        "            r'\\b(it is important to note|it should be noted|it is worth mentioning|it is crucial to understand)\\b',\n",
        "            r'\\b(In conclusion|To summarize|In summary|Overall|To conclude|In essence)\\b',\n",
        "            r'\\b(various|numerous|several|multiple|diverse|wide range of|extensive|comprehensive)\\b',\n",
        "            r'\\b(significant|substantial|considerable|notable|remarkable|exceptional|profound)\\b',\n",
        "            r'\\b(facilitate|utilize|implement|demonstrate|establish|maintain|ensure|optimize)\\b',\n",
        "            r'\\b(approach|methodology|framework|paradigm|concept|principle|strategy)\\b',\n",
        "            r'\\b(enables|allows|permits|provides|offers|presents|delivers)\\b',\n",
        "            r'\\b(particularly|specifically|especially|notably|remarkably)\\b',\n",
        "            r'\\b(fundamental|essential|critical|vital|crucial|imperative)\\b'\n",
        "        ]\n",
        "\n",
        "        # Aggressive word replacements learned from human text\n",
        "        self.replacements = {\n",
        "            \"furthermore\": [\"also\", \"plus\", \"and\", \"what's more\", \"besides\"],\n",
        "            \"moreover\": [\"also\", \"plus\", \"and\", \"besides\", \"on top of that\"],\n",
        "            \"additionally\": [\"also\", \"plus\", \"and\", \"too\", \"as well\"],\n",
        "            \"however\": [\"but\", \"though\", \"still\", \"yet\", \"although\"],\n",
        "            \"nevertheless\": [\"but\", \"still\", \"even so\", \"anyway\", \"regardless\"],\n",
        "            \"consequently\": [\"so\", \"therefore\", \"as a result\", \"because of this\", \"this means\"],\n",
        "            \"therefore\": [\"so\", \"thus\", \"that's why\", \"this means\", \"hence\"],\n",
        "            \"various\": [\"different\", \"many\", \"lots of\", \"all kinds of\", \"several\"],\n",
        "            \"numerous\": [\"many\", \"lots of\", \"tons of\", \"plenty of\", \"countless\"],\n",
        "            \"several\": [\"some\", \"a few\", \"many\", \"various\", \"multiple\"],\n",
        "            \"multiple\": [\"many\", \"lots of\", \"various\", \"different\", \"several\"],\n",
        "            \"comprehensive\": [\"complete\", \"full\", \"thorough\", \"detailed\", \"extensive\"],\n",
        "            \"extensive\": [\"wide\", \"broad\", \"large\", \"big\", \"vast\"],\n",
        "            \"significant\": [\"big\", \"major\", \"important\", \"huge\", \"substantial\"],\n",
        "            \"substantial\": [\"large\", \"big\", \"major\", \"considerable\", \"significant\"],\n",
        "            \"facilitate\": [\"help\", \"make easier\", \"enable\", \"assist\", \"support\"],\n",
        "            \"utilize\": [\"use\", \"employ\", \"work with\", \"apply\", \"leverage\"],\n",
        "            \"implement\": [\"put in place\", \"set up\", \"carry out\", \"execute\", \"apply\"],\n",
        "            \"demonstrate\": [\"show\", \"prove\", \"illustrate\", \"display\", \"reveal\"],\n",
        "            \"establish\": [\"set up\", \"create\", \"build\", \"form\", \"develop\"],\n",
        "            \"maintain\": [\"keep\", \"preserve\", \"sustain\", \"uphold\", \"continue\"],\n",
        "            \"ensure\": [\"make sure\", \"guarantee\", \"see to it\", \"confirm\", \"verify\"],\n",
        "            \"optimize\": [\"improve\", \"enhance\", \"better\", \"refine\", \"perfect\"],\n",
        "            \"approach\": [\"way\", \"method\", \"strategy\", \"technique\", \"manner\"],\n",
        "            \"methodology\": [\"method\", \"approach\", \"way\", \"system\", \"process\"],\n",
        "            \"framework\": [\"structure\", \"system\", \"setup\", \"foundation\", \"base\"],\n",
        "            \"paradigm\": [\"model\", \"approach\", \"way of thinking\", \"perspective\", \"viewpoint\"],\n",
        "            \"concept\": [\"idea\", \"notion\", \"thought\", \"principle\", \"theory\"],\n",
        "            \"principle\": [\"rule\", \"guideline\", \"basic idea\", \"foundation\", \"basis\"],\n",
        "            \"enables\": [\"lets\", \"allows\", \"makes possible\", \"permits\", \"helps\"],\n",
        "            \"allows\": [\"lets\", \"permits\", \"makes possible\", \"enables\", \"gives\"],\n",
        "            \"permits\": [\"allows\", \"lets\", \"enables\", \"makes possible\", \"gives\"],\n",
        "            \"provides\": [\"gives\", \"offers\", \"supplies\", \"delivers\", \"presents\"],\n",
        "            \"offers\": [\"gives\", \"provides\", \"presents\", \"supplies\", \"delivers\"],\n",
        "            \"presents\": [\"shows\", \"gives\", \"offers\", \"displays\", \"provides\"],\n",
        "            \"particularly\": [\"especially\", \"really\", \"very\", \"quite\", \"pretty\"],\n",
        "            \"specifically\": [\"especially\", \"particularly\", \"in particular\", \"mainly\", \"chiefly\"],\n",
        "            \"especially\": [\"particularly\", \"really\", \"very\", \"quite\", \"mainly\"],\n",
        "            \"notably\": [\"especially\", \"particularly\", \"remarkably\", \"significantly\", \"importantly\"],\n",
        "            \"fundamental\": [\"basic\", \"essential\", \"key\", \"core\", \"main\"],\n",
        "            \"essential\": [\"key\", \"important\", \"crucial\", \"vital\", \"necessary\"],\n",
        "            \"critical\": [\"important\", \"crucial\", \"key\", \"vital\", \"essential\"],\n",
        "            \"vital\": [\"important\", \"crucial\", \"essential\", \"key\", \"critical\"],\n",
        "            \"crucial\": [\"important\", \"key\", \"vital\", \"essential\", \"critical\"],\n",
        "            \"imperative\": [\"important\", \"essential\", \"crucial\", \"necessary\", \"vital\"]\n",
        "        }\n",
        "\n",
        "        # Enhanced contractions\n",
        "        self.contractions = {\n",
        "            \"do not\": \"don't\", \"does not\": \"doesn't\", \"did not\": \"didn't\",\n",
        "            \"can not\": \"can't\", \"cannot\": \"can't\", \"could not\": \"couldn't\",\n",
        "            \"would not\": \"wouldn't\", \"should not\": \"shouldn't\", \"will not\": \"won't\",\n",
        "            \"are not\": \"aren't\", \"is not\": \"isn't\", \"was not\": \"wasn't\",\n",
        "            \"were not\": \"weren't\", \"have not\": \"haven't\", \"has not\": \"hasn't\",\n",
        "            \"had not\": \"hadn't\", \"I am\": \"I'm\", \"you are\": \"you're\",\n",
        "            \"we are\": \"we're\", \"they are\": \"they're\", \"I will\": \"I'll\",\n",
        "            \"you will\": \"you'll\", \"we will\": \"we'll\", \"they will\": \"they'll\",\n",
        "            \"I have\": \"I've\", \"you have\": \"you've\", \"we have\": \"we've\",\n",
        "            \"they have\": \"they've\", \"that is\": \"that's\", \"there is\": \"there's\",\n",
        "            \"here is\": \"here's\", \"what is\": \"what's\", \"where is\": \"where's\",\n",
        "            \"who is\": \"who's\", \"how is\": \"how's\", \"it is\": \"it's\",\n",
        "            \"he is\": \"he's\", \"she is\": \"she's\", \"let us\": \"let's\"\n",
        "        }\n",
        "\n",
        "        # Load and learn from human datasets\n",
        "        if load_datasets:\n",
        "            self.load_human_datasets()\n",
        "\n",
        "        # print(\"âœ… Humanizer initialized successfully!\")\n",
        "\n",
        "    def load_human_datasets(self):\n",
        "        \"\"\"Load and learn patterns from human-written text datasets\"\"\"\n",
        "        # print(\"ðŸ“š Loading human writing datasets...\")\n",
        "\n",
        "        try:\n",
        "            # Load Reddit conversational data\n",
        "            # print(\"Loading Reddit data...\")\n",
        "            reddit_data = load_dataset(\"reddit_tifu\", \"short\", split=\"train[:500]\")\n",
        "            reddit_texts = [doc for doc in reddit_data['documents'] if len(doc) > 50]\n",
        "\n",
        "            # Load blog data for personal writing style\n",
        "            # print(\"Loading blog data...\")\n",
        "            try:\n",
        "                blog_data = load_dataset(\"blog_authorship_corpus\", split=\"train[:300]\")\n",
        "                blog_texts = [text for text in blog_data['text'] if len(text) > 50]\n",
        "            except:\n",
        "                blog_texts = []\n",
        "                # print(\"Blog dataset not available, skipping...\")\n",
        "\n",
        "            # Load news data for professional human writing\n",
        "            # print(\"Loading news data...\")\n",
        "            try:\n",
        "                news_data = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:200]\")\n",
        "                news_texts = [article for article in news_data['article'] if len(article) > 100]\n",
        "            except:\n",
        "                news_texts = []\n",
        "                # print(\"News dataset not available, skipping...\")\n",
        "\n",
        "            # Load Wikipedia for natural encyclopedic writing\n",
        "            # print(\"Loading Wikipedia data...\")\n",
        "            try:\n",
        "                wiki_data = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:300]\")\n",
        "                wiki_texts = [text for text in wiki_data['text'] if len(text) > 100]\n",
        "            except:\n",
        "                wiki_texts = []\n",
        "                # print(\"Wikipedia dataset not available, skipping...\")\n",
        "\n",
        "            # Combine all human texts\n",
        "            all_human_texts = reddit_texts + blog_texts + news_texts + wiki_texts\n",
        "\n",
        "            # Learn patterns from human text\n",
        "            self.learn_human_patterns(all_human_texts)\n",
        "\n",
        "            print(f\"âœ… Learned patterns from {len(all_human_texts)} human texts\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"âš ï¸ Error loading datasets: {e}\")\n",
        "            # print(\"Using default patterns...\")\n",
        "            self.set_default_patterns()\n",
        "\n",
        "    def learn_human_patterns(self, human_texts):\n",
        "        \"\"\"Extract patterns from human-written text\"\"\"\n",
        "        sentence_starters = []\n",
        "        connectors = []\n",
        "        casual_words = []\n",
        "        conversation_markers = []\n",
        "        personal_expressions = []\n",
        "\n",
        "        for text in human_texts[:100]:  # Sample for performance\n",
        "            try:\n",
        "                sentences = nltk.sent_tokenize(text)\n",
        "                for sentence in sentences:\n",
        "                    words = sentence.split()\n",
        "                    if len(words) > 3:\n",
        "                        # Collect sentence starters\n",
        "                        starter = ' '.join(words[:2]).lower()\n",
        "                        if starter not in ['the', 'a', 'an', 'this', 'that', 'it', 'he', 'she']:\n",
        "                            sentence_starters.append(starter)\n",
        "\n",
        "                        # Look for conversational markers\n",
        "                        sentence_lower = sentence.lower()\n",
        "                        if any(marker in sentence_lower for marker in ['i think', 'i believe', 'in my', 'personally']):\n",
        "                            personal_expressions.append(sentence[:50])\n",
        "\n",
        "                        # Find casual connectors\n",
        "                        for word in ['but', 'and', 'so', 'plus', 'also', 'though']:\n",
        "                            if sentence_lower.startswith(word + ' '):\n",
        "                                connectors.append(word)\n",
        "\n",
        "                        # Collect casual words\n",
        "                        casual_indicators = ['really', 'pretty', 'quite', 'actually', 'honestly', 'basically']\n",
        "                        for word in casual_indicators:\n",
        "                            if word in sentence_lower:\n",
        "                                casual_words.append(word)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Update patterns with most common ones\n",
        "        self.human_patterns['sentence_starters'] = list(set(sentence_starters))[:30]\n",
        "        self.human_patterns['connectors'] = list(set(connectors))\n",
        "        self.human_patterns['casual_words'] = list(set(casual_words))\n",
        "        self.human_patterns['personal_expressions'] = list(set(personal_expressions))[:20]\n",
        "\n",
        "    def set_default_patterns(self):\n",
        "        \"\"\"Set default human patterns if dataset loading fails\"\"\"\n",
        "        self.human_patterns = {\n",
        "            'sentence_starters': [\n",
        "                \"honestly\", \"actually\", \"look\", \"listen\", \"you know\", \"i think\",\n",
        "                \"from what\", \"in my\", \"personally\", \"frankly\", \"to be\", \"the way\"\n",
        "            ],\n",
        "            'connectors': [\"but\", \"and\", \"so\", \"plus\", \"also\", \"though\", \"still\"],\n",
        "            'casual_words': [\"really\", \"pretty\", \"quite\", \"actually\", \"honestly\", \"basically\"],\n",
        "            'personal_expressions': [\n",
        "                \"honestly\", \"in my opinion\", \"from what I've seen\", \"personally\",\n",
        "                \"if you ask me\", \"the way I see it\", \"from my experience\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def lazy_load_paraphraser(self):\n",
        "        \"\"\"Load paraphraser only when needed\"\"\"\n",
        "        if self.paraphraser is None:\n",
        "            try:\n",
        "                # print(\"Loading paraphrasing model...\")\n",
        "                self.paraphraser = pipeline(\"text2text-generation\",\n",
        "                                           model=\"Vamsi/T5_Paraphrase_Paws\",\n",
        "                                           max_length=512)\n",
        "                print(\"âœ… Paraphraser loaded\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Could not load paraphraser: {e}\")\n",
        "                self.paraphraser = False\n",
        "        return self.paraphraser\n",
        "\n",
        "    def detect_ai_patterns(self, text):\n",
        "        \"\"\"Enhanced AI pattern detection\"\"\"\n",
        "        score = 0\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        # Pattern matching\n",
        "        for pattern in self.ai_patterns:\n",
        "            matches = len(re.findall(pattern, text, re.IGNORECASE))\n",
        "            score += matches * 0.5\n",
        "\n",
        "        # Sentence structure analysis\n",
        "        if len(sentences) > 3:\n",
        "            lengths = [len(s.split()) for s in sentences]\n",
        "            avg_length = np.mean(lengths)\n",
        "            variance = np.var(lengths)\n",
        "\n",
        "            # Penalize very uniform sentence lengths\n",
        "            if variance < 5:\n",
        "                score += 1\n",
        "\n",
        "            # Penalize overly long sentences\n",
        "            if avg_length > 25:\n",
        "                score += 0.5\n",
        "\n",
        "        # Check for repetitive sentence starters\n",
        "        starters = []\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            if words:\n",
        "                starters.append(words[0].lower())\n",
        "\n",
        "        if len(set(starters)) / max(len(starters), 1) < 0.6:\n",
        "            score += 0.5\n",
        "\n",
        "        return score / max(len(sentences), 1)\n",
        "\n",
        "    def aggressive_pattern_removal(self, text):\n",
        "        \"\"\"Remove AI patterns aggressively\"\"\"\n",
        "        replacements = {\n",
        "            r'\\bFurthermore,?\\s*': random.choice(['Also, ', 'Plus, ', 'And ', 'What\\'s more, ', '']),\n",
        "            r'\\bMoreover,?\\s*': random.choice(['Also, ', 'Plus, ', 'And ', 'Besides, ', '']),\n",
        "            r'\\bAdditionally,?\\s*': random.choice(['Also, ', 'Plus, ', 'And ', 'Too, ', '']),\n",
        "            r'\\bHowever,?\\s*': random.choice(['But ', 'Though ', 'Still, ', 'Yet ', '']),\n",
        "            r'\\bNevertheless,?\\s*': random.choice(['But ', 'Still, ', 'Even so, ', 'Anyway, ', '']),\n",
        "            r'\\bConsequently,?\\s*': random.choice(['So ', 'This means ', 'Because of this, ', 'As a result, ', '']),\n",
        "            r'\\bTherefore,?\\s*': random.choice(['So ', 'That\\'s why ', 'This means ', 'Hence ', '']),\n",
        "            r'\\bIn conclusion,?\\s*': random.choice(['So, ', 'Bottom line: ', 'To wrap up, ', 'Overall, ', '']),\n",
        "            r'\\bTo summarize,?\\s*': random.choice(['In short, ', 'Basically, ', 'So, ', 'To sum up, ', '']),\n",
        "            r'\\bit is important to note that\\s*': random.choice(['Note that ', 'Keep in mind ', 'Remember, ', '']),\n",
        "            r'\\bit should be noted that\\s*': random.choice(['Remember, ', 'Note that ', 'Keep in mind ', '']),\n",
        "            r'\\bit is worth mentioning that\\s*': random.choice(['Also, ', 'By the way, ', 'Worth noting: ', '']),\n",
        "            r'\\bit is crucial to understand that\\s*': random.choice(['You need to know ', 'Remember, ', 'Keep in mind ', '']),\n",
        "            r'\\bIn essence,?\\s*': random.choice(['Basically, ', 'Simply put, ', 'In short, ', '']),\n",
        "            r'\\bOverall,?\\s*': random.choice(['Generally, ', 'All in all, ', 'In the end, ', ''])\n",
        "        }\n",
        "\n",
        "        for pattern, replacement in replacements.items():\n",
        "            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def aggressive_word_replacement(self, text):\n",
        "        \"\"\"Replace formal words with casual alternatives\"\"\"\n",
        "        words = text.split()\n",
        "        new_words = []\n",
        "\n",
        "        for word in words:\n",
        "            clean_word = word.lower().strip(string.punctuation)\n",
        "\n",
        "            # High replacement rate for better humanization\n",
        "            if clean_word in self.replacements and random.random() < 0.8:\n",
        "                replacement = random.choice(self.replacements[clean_word])\n",
        "\n",
        "                # Preserve capitalization\n",
        "                if word[0].isupper():\n",
        "                    replacement = replacement.capitalize()\n",
        "\n",
        "                # Add back punctuation\n",
        "                punct = ''.join([c for c in word if c in string.punctuation])\n",
        "                new_words.append(replacement + punct)\n",
        "            else:\n",
        "                new_words.append(word)\n",
        "\n",
        "        return ' '.join(new_words)\n",
        "\n",
        "    def add_human_personality(self, text):\n",
        "        \"\"\"Add strong human personality markers\"\"\"\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        new_sentences = []\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            # Add learned human starters\n",
        "            if i > 0 and random.random() < 0.4:\n",
        "                if self.human_patterns['sentence_starters']:\n",
        "                    starter = random.choice(self.human_patterns['sentence_starters'])\n",
        "                    if not sentence.lower().startswith(('and', 'but', 'or', 'so')):\n",
        "                        sentence = starter.capitalize() + ', ' + sentence.lower()\n",
        "\n",
        "            # Add casual interjections from learned patterns\n",
        "            if random.random() < 0.35:\n",
        "                if self.human_patterns['casual_words']:\n",
        "                    interjection = random.choice(self.human_patterns['casual_words'])\n",
        "                    words = sentence.split()\n",
        "                    if len(words) > 3:\n",
        "                        pos = random.randint(1, min(3, len(words)-1))\n",
        "                        words.insert(pos, interjection)\n",
        "                        sentence = ' '.join(words)\n",
        "\n",
        "            # Add personal expressions occasionally\n",
        "            if random.random() < 0.15 and self.human_patterns['personal_expressions']:\n",
        "                personal_expr = random.choice(self.human_patterns['personal_expressions'])\n",
        "                sentence = personal_expr + ', ' + sentence.lower()\n",
        "\n",
        "            new_sentences.append(sentence)\n",
        "\n",
        "        return ' '.join(new_sentences)\n",
        "\n",
        "    def add_contractions(self, text):\n",
        "        \"\"\"Convert formal phrases to contractions\"\"\"\n",
        "        for formal, contraction in self.contractions.items():\n",
        "            text = re.sub(r'\\b' + formal + r'\\b', contraction, text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "\n",
        "    def break_formal_structure(self, text):\n",
        "        \"\"\"Break formal sentence structures\"\"\"\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        new_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "\n",
        "            # Break very long sentences (18+ words)\n",
        "            if len(words) > 18:\n",
        "                # Find natural break points\n",
        "                break_points = []\n",
        "                for i, word in enumerate(words):\n",
        "                    if word.lower() in ['and', 'but', 'or', 'because', 'since', 'while', 'when', 'although']:\n",
        "                        break_points.append(i)\n",
        "\n",
        "                if break_points and random.random() < 0.6:\n",
        "                    break_point = random.choice(break_points)\n",
        "                    first_part = ' '.join(words[:break_point]) + '.'\n",
        "                    second_part = ' '.join(words[break_point:])\n",
        "                    new_sentences.extend([first_part, second_part])\n",
        "                else:\n",
        "                    new_sentences.append(sentence)\n",
        "            else:\n",
        "                new_sentences.append(sentence)\n",
        "\n",
        "        return ' '.join(new_sentences)\n",
        "\n",
        "    def add_conversational_elements(self, text):\n",
        "        \"\"\"Add conversational elements\"\"\"\n",
        "        # Add rhetorical questions occasionally\n",
        "        if random.random() < 0.25:\n",
        "            questions = [\"You know what I mean?\", \"Right?\", \"Make sense?\", \"See what I'm getting at?\", \"Know what I'm saying?\"]\n",
        "            text += \" \" + random.choice(questions)\n",
        "\n",
        "        # Add casual expressions\n",
        "        casual_expressions = [\n",
        "            \" (which is pretty cool)\", \" (if you ask me)\", \" (honestly)\",\n",
        "            \" (at least that's what I think)\", \" (from my experience)\",\n",
        "            \" (personally speaking)\", \" (in my opinion)\"\n",
        "        ]\n",
        "\n",
        "        if random.random() < 0.3:\n",
        "            sentences = text.split('.')\n",
        "            if len(sentences) > 2:\n",
        "                insert_pos = random.randint(1, len(sentences)-2)\n",
        "                sentences[insert_pos] += random.choice(casual_expressions)\n",
        "                text = '.'.join(sentences)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def advanced_paraphrase(self, text):\n",
        "        \"\"\"Use transformer model for paraphrasing\"\"\"\n",
        "        paraphraser = self.lazy_load_paraphraser()\n",
        "        if not paraphraser:\n",
        "            return text\n",
        "\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        paraphrased_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Only paraphrase very AI-like sentences\n",
        "            if len(sentence.split()) > 12 and random.random() < 0.3:\n",
        "                try:\n",
        "                    paraphrase_input = f\"paraphrase: {sentence}\"\n",
        "                    result = paraphraser(paraphrase_input,\n",
        "                                        max_length=min(len(sentence.split()) + 20, 150),\n",
        "                                        do_sample=True,\n",
        "                                        temperature=0.8,\n",
        "                                        num_return_sequences=1)\n",
        "                    paraphrased = result[0]['generated_text']\n",
        "                    paraphrased_sentences.append(paraphrased)\n",
        "                except Exception as e:\n",
        "                    paraphrased_sentences.append(sentence)\n",
        "            else:\n",
        "                paraphrased_sentences.append(sentence)\n",
        "\n",
        "        return ' '.join(paraphrased_sentences)\n",
        "\n",
        "    def quality_check_and_fix(self, text):\n",
        "        \"\"\"Fix quality issues\"\"\"\n",
        "        # Fix spacing\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'\\s+([,.!?;:])', r'\\1', text)\n",
        "\n",
        "        # Fix capitalization\n",
        "        text = re.sub(r'(\\.)(\\s*)([a-z])', lambda m: m.group(1) + m.group(2) + m.group(3).upper(), text)\n",
        "\n",
        "        # Remove awkward combinations\n",
        "        awkward_patterns = [\n",
        "            r'\\broughly a\\b', r'\\bbasically totally\\b', r'\\bPlus, Note\\b',\n",
        "            r'\\bAnd And\\b', r'\\bBut But\\b', r'\\bSo So\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in awkward_patterns:\n",
        "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Fix incomplete sentences\n",
        "        text = re.sub(r'\\.\\s*\\.\\s*', '. ', text)\n",
        "        text = re.sub(r'\\s+\\.', '.', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def calculate_quality_metrics(self, text):\n",
        "        \"\"\"Calculate comprehensive quality metrics\"\"\"\n",
        "        try:\n",
        "            readability = flesch_reading_ease(text)\n",
        "            grade_level = flesch_kincaid_grade(text)\n",
        "\n",
        "            # Readability score (0-100, higher is better)\n",
        "            if readability < 0:\n",
        "                readability_score = 0\n",
        "            elif readability < 30:\n",
        "                readability_score = 25\n",
        "            elif readability < 50:\n",
        "                readability_score = 50\n",
        "            elif readability < 70:\n",
        "                readability_score = 75\n",
        "            else:\n",
        "                readability_score = 100\n",
        "\n",
        "            # Grammar and flow score\n",
        "            sentences = nltk.sent_tokenize(text)\n",
        "            grammar_score = 100\n",
        "\n",
        "            # Check for common issues\n",
        "            if re.search(r'\\b(roughly a|basically totally|Plus, Note)\\b', text, re.IGNORECASE):\n",
        "                grammar_score -= 30\n",
        "\n",
        "            if len(sentences) > 0:\n",
        "                avg_length = np.mean([len(s.split()) for s in sentences])\n",
        "                if avg_length > 30:  # Very long sentences\n",
        "                    grammar_score -= 20\n",
        "                elif avg_length < 5:  # Very short sentences\n",
        "                    grammar_score -= 10\n",
        "\n",
        "            # Overall quality\n",
        "            overall_quality = (readability_score + grammar_score) / 2\n",
        "\n",
        "            return {\n",
        "                'readability': readability,\n",
        "                'grade_level': grade_level,\n",
        "                'readability_score': readability_score,\n",
        "                'grammar_score': max(0, grammar_score),\n",
        "                'overall_quality': max(0, overall_quality)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'readability': 50,\n",
        "                'grade_level': 10,\n",
        "                'readability_score': 50,\n",
        "                'grammar_score': 50,\n",
        "                'overall_quality': 50\n",
        "            }\n",
        "\n",
        "    def humanize(self, text, intensity='maximum'):\n",
        "        \"\"\"Main humanization function with maximum effectiveness\"\"\"\n",
        "        print(\"ðŸš€ Starting advanced humanization process...\")\n",
        "\n",
        "        # Calculate initial scores\n",
        "        initial_ai_score = self.detect_ai_patterns(text)\n",
        "        initial_quality = self.calculate_quality_metrics(text)\n",
        "\n",
        "        # print(f\"ðŸ“Š Initial AI-like score: {initial_ai_score:.2f}\")\n",
        "        # print(f\"ðŸ“Š Initial quality: {initial_quality['overall_quality']:.1f}%\")\n",
        "\n",
        "        # Apply aggressive transformations\n",
        "        current_text = text\n",
        "\n",
        "        transformations = [\n",
        "            (\"\", self.aggressive_pattern_removal),\n",
        "            (\"\", self.add_contractions),\n",
        "            (\"\", self.aggressive_word_replacement),\n",
        "            (\"\", self.add_human_personality),\n",
        "            (\"\", self.break_formal_structure),\n",
        "            (\"\", self.add_conversational_elements)\n",
        "        ]\n",
        "\n",
        "        if intensity == 'maximum':\n",
        "            transformations.append((\"Advanced paraphrasing\", self.advanced_paraphrase))\n",
        "\n",
        "        for desc, transform_func in transformations:\n",
        "            print(f\" {desc}\")\n",
        "            try:\n",
        "                current_text = transform_func(current_text)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error in {desc}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Final quality check\n",
        "        print(\"ðŸ”§ Final quality check...\")\n",
        "        current_text = self.quality_check_and_fix(current_text)\n",
        "\n",
        "        # Calculate final scores\n",
        "        final_ai_score = self.detect_ai_patterns(current_text)\n",
        "        final_quality = self.calculate_quality_metrics(current_text)\n",
        "\n",
        "        improvement = ((initial_ai_score - final_ai_score) / max(initial_ai_score, 0.1)) * 100\n",
        "\n",
        "        # print(f\"ðŸ“ˆ Final AI-like score: {final_ai_score:.2f}\")\n",
        "        # print(f\"ðŸ“ˆ Final quality: {final_quality['overall_quality']:.1f}%\")\n",
        "        # print(f\"ðŸŽ¯ AI detection improvement: {improvement:.1f}%\")\n",
        "        print(\"âœ… Humanization complete!\")\n",
        "\n",
        "        return current_text, {\n",
        "            'initial_ai_score': initial_ai_score,\n",
        "            'final_ai_score': final_ai_score,\n",
        "            'improvement': improvement,\n",
        "            'initial_quality': initial_quality,\n",
        "            'final_quality': final_quality,\n",
        "            'target_achieved': final_ai_score < 0.5  # Target: <50% of original AI score\n",
        "        }\n",
        "\n",
        "    def batch_humanize(self, texts, intensity='maximum'):\n",
        "        \"\"\"Humanize multiple texts efficiently\"\"\"\n",
        "        results = []\n",
        "        for i, text in enumerate(texts):\n",
        "            # print(f\"\\nðŸ“ Processing text {i+1}/{len(texts)}...\")\n",
        "            try:\n",
        "                humanized, stats = self.humanize(text, intensity)\n",
        "                results.append({\n",
        "                    'original': text,\n",
        "                    'humanized': humanized,\n",
        "                    'stats': stats\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error processing text {i+1}: {e}\")\n",
        "                results.append({\n",
        "                    'original': text,\n",
        "                    'humanized': text,\n",
        "                    'stats': None\n",
        "                })\n",
        "        return results\n",
        "\n",
        "# Initialize the humanizer with dataset training\n",
        "# print(\"ðŸŽ¯ Initializing AI Text Humanizer\")\n",
        "humanizer = AdvancedAITextHumanizer(load_datasets=True)\n",
        "\n",
        "# Test with sample AI text\n",
        "textToBeHumanised = input('Enter Text you want to Humanize:')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "# print(\"TESTING WITH SAMPLE TEXT\")\n",
        "# print(\"=\"*60)\n",
        "\n",
        "# print(\"\\nOriginal Text:\")\n",
        "# print(\"-\" * 40)\n",
        "# print(sample_ai_text)\n",
        "\n",
        "# Humanize the text\n",
        "humanized_text, stats = humanizer.humanize(sample_ai_text, intensity='maximum')\n",
        "\n",
        "print(\"\\nHumanized Text:\")\n",
        "print(\"-\" * 40)\n",
        "print(humanized_text)\n",
        "\n",
        "# print(f\"\\nðŸ“Š RESULTS:\")\n",
        "# print(f\"AI detection improved by: {stats['improvement']:.1f}%\")\n",
        "# print(f\"Quality score: {stats['final_quality']['overall_quality']:.1f}%\")\n",
        "# print(f\"Readability: {stats['final_quality']['readability']:.1f}\")\n",
        "# print(f\"Target achieved: {'âœ… YES' if stats['target_achieved'] else 'âŒ NO'}\")\n",
        "\n",
        "# Function to humanize custom text\n",
        "def humanize_custom_text():\n",
        "    \"\"\"Interactive function to humanize user's text\"\"\"\n",
        "    print(\"\\nðŸš€ Ready to humanize your text!\")\n",
        "    print(\"Paste your AI-generated text below (press Enter twice when done):\")\n",
        "\n",
        "    lines = []\n",
        "    empty_count = 0\n",
        "    while empty_count < 2:\n",
        "        line = input()\n",
        "        if line == \"\":\n",
        "            empty_count += 1\n",
        "        else:\n",
        "            empty_count = 0\n",
        "            lines.append(line)\n",
        "\n",
        "    if not lines:\n",
        "        print(\"âŒ No text provided!\")\n",
        "        return\n",
        "\n",
        "    user_text = '\\n'.join(lines)\n",
        "\n",
        "    # print(\"\\nChoose intensity level:\")\n",
        "    # print(\"1. High (recommended)\")\n",
        "    # print(\"2. Maximum (most aggressive)\")\n",
        "\n",
        "    choice = input(\"Enter choice (1-2): \").strip()\n",
        "    intensity = 'maximum' if choice == '2' else 'high'\n",
        "\n",
        "    # print(f\"\\nðŸ”„ Processing with {intensity} intensity...\")\n",
        "    humanized, stats = humanizer.humanize(user_text, intensity=intensity)\n",
        "\n",
        "    print(\"\\nðŸŽ‰ Your humanized text:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(humanized)\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(f\"\\nðŸ“ˆ Results:\")\n",
        "    print(f\"AI detection improved by: {stats['improvement']:.1f}%\")\n",
        "    print(f\"Quality score: {stats['final_quality']['overall_quality']:.1f}%\")\n",
        "    print(f\"Target achieved: {'âœ… YES' if stats['target_achieved'] else 'âŒ NO'}\")\n",
        "\n",
        "    if stats['target_achieved']:\n",
        "        print(\"ðŸŽ¯ Great! Your text should now pass most AI detectors.\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Consider running the text through the humanizer again for better results.\")\n",
        "\n",
        "# Uncomment to use interactive mode\n",
        "# humanize_custom_text()\n",
        "\n",
        "print(\"\\nðŸŽ‰ AI Text Humanizer is ready!\")\n",
        "# print(\"Call humanize_custom_text() to start humanizing your text interactively.\")\n",
        "# print(\"Or use humanizer.humanize(your_text, intensity='maximum') directly.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5wgC0D5sDGHE",
        "outputId": "a1207fa5-b964-4cc6-9f2d-8e040c970748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.7-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.0.33-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textstat-0.7.7-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.0.33-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hWith the rapid advancement of artificial intelligence, tools like ChatGPT and other language models have become increasingly adept at writing essays, stories, and reports. However, these AI-generated texts often carry subtle patternsâ€”certain word choices, sentence structures, or overly formal tonesâ€”that can reveal their non-human origin.  This is where AI Text Humanizers step in.  What is an AI Text Humanizer? An AI Text Humanizer is a tool designed to rewrite or tweak AI-generated content so it sounds like it was written by a real human. It subtly alters the tone, phrasing, and structure of the text to remove the robotic or \"too perfect\" feel that AI often produces. The result is content that feels more natural, emotional, and personalâ€”like something youâ€™d expect from a friend, student, or professional writer.  Why is this Important? Many websites and institutions now use AI detection software to identify whether content was generated by AI. Students, job applicants, or freelance writers using AI tools risk being flagged or penalized. Humanizers help them avoid detection by making the content seem authentically human.  How Does It Work? Behind the scenes, these tools use a combination of techniques:  Paraphrasing algorithms that reword AI-written text.  Statistical and neural models trained on real human writing to mimic natural styles.  Tone and emotion tuning to make the text feel more expressive or personal.  Some advanced models even test their output against popular AI detectors to ensure the rewritten text scores low on AI-likeness.  The Ethical Debate While AI text humanizers can be helpful for improving tone and clarity, they also raise ethical concerns. Should people be allowed to pass off AI-written work as their own? Are we blurring the line between assistance and deception?  As AI becomes more integrated into daily life, tools like the AI Text Humanizer are likely to spark ongoing debate in classrooms, workplaces, and courtrooms.\n",
            "\n",
            "Installing collected packages: pyphen, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, cmudict, textstat, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 455, in run\n",
            "    installed = install_given_reqs(\n",
            "                ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/__init__.py\", line 70, in install_given_reqs\n",
            "    requirement.install(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/req/req_install.py\", line 851, in install\n",
            "    install_wheel(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/install/wheel.py\", line 726, in install_wheel\n",
            "    _install_wheel(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/install/wheel.py\", line 584, in _install_wheel\n",
            "    file.save()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/install/wheel.py\", line 382, in save\n",
            "    shutil.copyfileobj(f, dest)\n",
            "  File \"/usr/lib/python3.11/shutil.py\", line 197, in copyfileobj\n",
            "    buf = fsrc_read(length)\n",
            "          ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/zipfile.py\", line 966, in read\n",
            "    data = self._read1(n)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/zipfile.py\", line 1041, in _read1\n",
            "    n = max(n, self.MIN_READ_SIZE)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1536, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1632, in _log\n",
            "    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1595, in makeRecord\n",
            "    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3013451360.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers torch nltk spacy textstat datasets scikit-learn pandas numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m spacy download en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}